# -*- coding: utf-8 -*-
"""Fine-Tuning (LlaMA).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ramfp5DcZA-ql0ZnFjLgn7NBSiNj7O-X
"""

!pip install -q unsloth

from unsloth import FastLanguageModel
import torch

MAX_SEQUENCE_LENGTH = 2048
D_TYPE = None # Depending on the GPU, auto-configs it
LOAD_IN_4BIT = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = 'unsloth/Llama-3.2-3B-Instruct',
    max_seq_length = MAX_SEQUENCE_LENGTH, # Context-window length
    dtype = D_TYPE,
    load_in_4bit = LOAD_IN_4BIT
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None
)

from datasets import load_dataset
dataset = load_dataset("ServiceNow-AI/R1-Distill-SFT", 'v0', split="train")

import pandas as pd

x = pd.DataFrame(dataset)
x[:10]

prompt = """You are a reflective assistant engaging in thoroughly, iterative reasoning, mimicking human stream of consciousness thinking. Your approach emphasizes exploration, self-doubt, and continous refinement before coming up with an answer.
<problem>
{}
</problem>

{}
{}
"""
EOS_TOKEN = tokenizer.eos_token

def formatting_prompt(examples):
  problems = examples["problem"]
  thoughts = examples["reannotated_assistant_content"]
  solutions = examples["solution"]
  texts = []

  for problem, thought, solution in zip(problems, thoughts, solutions):
    text = prompt.format(problem, thought, solution)+EOS_TOKEN
    texts.append(text)


  return {"text": texts}

dataset = dataset.map(formatting_prompt, batched = True)

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = MAX_SEQUENCE_LENGTH,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "fine-tuned-output",
        report_to = "none",
    ),
)

trainer_stats = trainer.train()

from unsloth.chat_templates import get_chat_template
from transformers import TextIteratorStreamer
import threading

sys_prompt = """You are a reflective assistant engaging in thoroughly, iterative reasoning, mimicking human stream of consciousness thinking. Your approach emphasizes exploration, self-doubt, and continous refinement before coming up with an answer. Think internally and provide only the final answer.
<problem>
{}
</problem>
"""

message = sys_prompt.format("How many r's are present in the word 'strawberry?'")
tokenizer = get_chat_template(
  tokenizer,
  chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)

messages = [
    {"role": "user", "content": message},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt=True,
    return_tensors="pt",
).to("cuda")

streamer = TextIteratorStreamer(
    tokenizer,
    skip_prompt=True,
    skip_special_tokens=True,
)

generation_kwargs = dict(
    input_ids=inputs,
    max_new_tokens=1024,
    use_cache=True,
    temperature=1.5,
    min_p=0.1,
    streamer=streamer,
)

thread = threading.Thread(
    target=model.generate,
    kwargs=generation_kwargs,
)
thread.start()

for token in streamer:
    print(token, end="", flush=True)

# outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True, temperature=1.5, min_p =0.1)
# response = tokenizer.batch_decode(outputs)

print(response[0])

save_dir = "./llama3-3B-KServe"

model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

from huggingface_hub import login, upload_folder
login()

upload_folder(folder_path="./llama3-3B-KServe", repo_id="<YOUR_REPO_ID>", repo_type="model")

